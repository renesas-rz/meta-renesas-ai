From 45ba261e1c35bc06ec38fcc3fdc32a5223d1dac7 Mon Sep 17 00:00:00 2001
From: Ayman Chaudhry <ayman.chaudhry.kc@renesas.com>
Date: Tue, 16 Apr 2024 15:41:45 +0100
Subject: [PATCH] Add generic Arm NN SDK inference framework and test code

A C++ framework and test that is able to seperate the original
Armnn test framework class into following four independent steps:
1. Image processing
2. Model loading
3. Inferencing
4. Postprocessing of the prediction results

Signed-off-by: Jianming Qiao <jianming.qiao@bp.renesas.com>
Signed-off-by: Gareth Williams <gareth.williams.jx@renesas.com>
Signed-off-by: Ayman Chaudhry <ayman.chaudhry.kc@renesas.com>
---
 tests/CMakeLists.txt                          |  21 +
 tests/InferenceTest.hpp                       |  47 +++
 .../RenesasSample-Armnn.cpp                   | 384 ++++++++++++++++++
 3 files changed, 452 insertions(+)
 create mode 100644 tests/RenesasSample-Armnn/RenesasSample-Armnn.cpp

diff --git a/tests/CMakeLists.txt b/tests/CMakeLists.txt
index eca03b1bd..b9aac5013 100644
--- a/tests/CMakeLists.txt
+++ b/tests/CMakeLists.txt
@@ -172,6 +172,18 @@ if (BUILD_ARMNN_SERIALIZER
                                                       ./NetworkExecutionUtils
                                                       ${CMAKE_CURRENT_SOURCE_DIR})
 
+    set(Renesas-Reference-Code-Armnn_sources
+	RenesasSample-Armnn/RenesasSample-Armnn.cpp
+	ImagePreprocessor.hpp
+	ImagePreprocessor.cpp
+	InferenceTestImage.hpp
+	InferenceTestImage.cpp)
+
+    add_executable_ex(RenesasSample-Armnn ${Renesas-Reference-Code-Armnn_sources})
+    target_include_directories(RenesasSample-Armnn PRIVATE ../src/armnn)
+    target_include_directories(RenesasSample-Armnn PRIVATE ../src/armnnUtils)
+    target_include_directories(RenesasSample-Armnn PRIVATE ../src/backends)
+
     if(EXECUTE_NETWORK_STATIC)
         target_link_libraries(ExecuteNetwork
                 -Wl,--whole-archive
@@ -184,12 +196,15 @@ if (BUILD_ARMNN_SERIALIZER
     else()
         if (BUILD_ARMNN_SERIALIZER)
             target_link_libraries(ExecuteNetwork armnnSerializer)
+            target_link_libraries(RenesasSample-Armnn armnnSerializer)
         endif()
         if (BUILD_TF_LITE_PARSER)
             target_link_libraries(ExecuteNetwork armnnTfLiteParser)
+            target_link_libraries(RenesasSample-Armnn armnnTfLiteParser)
         endif()
         if (BUILD_ONNX_PARSER)
             target_link_libraries(ExecuteNetwork armnnOnnxParser)
+            target_link_libraries(RenesasSample-Armnn armnnOnnxParser)
         endif()
         if (BUILD_CLASSIC_DELEGATE)
             target_link_libraries(ExecuteNetwork ArmnnDelegate::ArmnnDelegate)
@@ -202,6 +217,10 @@ if (BUILD_ARMNN_SERIALIZER
 
    target_link_libraries(ExecuteNetwork ${CMAKE_THREAD_LIBS_INIT})
    addDllCopyCommands(ExecuteNetwork)
+
+   target_link_libraries(RenesasSample-Armnn armnn)
+   target_link_libraries(RenesasSample-Armnn ${CMAKE_THREAD_LIBS_INIT})
+   addDllCopyCommands(RenesasSample-Armnn)
 endif()
 
 if(BUILD_ACCURACY_TOOL)
@@ -212,9 +231,11 @@ if(BUILD_ACCURACY_TOOL)
         endif()
         if (BUILD_TF_LITE_PARSER)
             target_link_libraries(${executorName} armnnTfLiteParser)
+            target_link_libraries(RenesasSample-Armnn armnnTfLiteParser)
         endif()
         if (BUILD_ONNX_PARSER)
             target_link_libraries(${executorName} armnnOnnxParser)
+            target_link_libraries(RenesasSample-Armnn armnnOnnxParser)
         endif()
         addDllCopyCommands(${executorName})
     endmacro()
diff --git a/tests/InferenceTest.hpp b/tests/InferenceTest.hpp
index f8a7d5e61..fb454b0a3 100644
--- a/tests/InferenceTest.hpp
+++ b/tests/InferenceTest.hpp
@@ -108,6 +108,53 @@ public:
     virtual bool OnInferenceTestFinished() { return true; };
 };
 
+template <typename TDataType>
+struct ToFloat { }; // nothing defined for the generic case
+
+template <>
+struct ToFloat<float>
+{
+    static inline float Convert(float value, const InferenceModelInternal::QuantizationParams &)
+    {
+        // assuming that float models are not quantized
+        return value;
+    }
+
+    static inline float Convert(int value, const InferenceModelInternal::QuantizationParams &)
+    {
+        // assuming that float models are not quantized
+        return static_cast<float>(value);
+    }
+};
+
+template <>
+struct ToFloat<uint8_t>
+{
+    static inline float Convert(uint8_t value,
+                                const InferenceModelInternal::QuantizationParams & quantizationParams)
+    {
+        return armnn::Dequantize<uint8_t>(value,
+                                          quantizationParams.first,
+                                          quantizationParams.second);
+    }
+
+    static inline float Convert(int value,
+                                const InferenceModelInternal::QuantizationParams & quantizationParams)
+    {
+        return armnn::Dequantize<uint8_t>(static_cast<uint8_t>(value),
+                                          quantizationParams.first,
+                                          quantizationParams.second);
+    }
+
+    static inline float Convert(float value,
+                                const InferenceModelInternal::QuantizationParams & quantizationParams)
+    {
+        return armnn::Dequantize<uint8_t>(static_cast<uint8_t>(value),
+                                          quantizationParams.first,
+                                          quantizationParams.second);
+    }
+};
+
 template <typename TModel>
 class InferenceModelTestCase : public IInferenceTestCase
 {
diff --git a/tests/RenesasSample-Armnn/RenesasSample-Armnn.cpp b/tests/RenesasSample-Armnn/RenesasSample-Armnn.cpp
new file mode 100644
index 000000000..7f958de83
--- /dev/null
+++ b/tests/RenesasSample-Armnn/RenesasSample-Armnn.cpp
@@ -0,0 +1,384 @@
+/*
+ * Copyright (C) 2021 Renesas Electronics Corp.
+ * This file is licensed under the terms of the MIT License
+ * This program is licensed "as is" without any warranty of any
+ * kind, whether express or implied.
+ */
+
+#include <armnn/ArmNN.hpp>
+#include <armnn/TypesUtils.hpp>
+
+#if defined(ARMNN_TF_LITE_PARSER)
+#include "armnnTfLiteParser/ITfLiteParser.hpp"
+#endif
+#if defined(ARMNN_ONNX_PARSER)
+#include "armnnOnnxParser/IOnnxParser.hpp"
+#endif
+
+#include "../InferenceTest.hpp"
+
+#include <armnn/Logging.hpp>
+#include <Profiling.hpp>
+#include "../ImagePreprocessor.hpp"
+#include "../InferenceTestImage.hpp"
+
+#include <iostream>
+#include <fstream>
+#include <functional>
+#include <future>
+#include <algorithm>
+#include <iterator>
+#include <numeric>
+
+#define NUMBER_RUN_TESTS 30
+
+std::map<int,std::string> label_file_map;
+
+string base_path = "/usr/bin/armnn/examples";
+
+typedef struct model_params {
+    std::string modelFormat;
+    bool isFloatModel;
+    std::string modelPath;
+    armnn::TensorShape inputTensorShape;
+    std::string inputName;
+    std::string outputName;
+    unsigned int inputImageWidth;
+    unsigned int inputImageHeight;
+} model_params;
+
+std::map<std::string, model_params> Model_Table;
+
+template <typename TDataType>
+int ProcessResult(std::vector<TDataType>& output, InferenceModelInternal::QuantizationParams quantParams, const string mode_type)
+{
+    std::map<float,int> resultMap;
+
+    int index = 0;
+
+    for (const auto & o : output)
+    {
+        float prob = ToFloat<TDataType>::Convert(o, quantParams);
+        int classification = index++;
+
+        std::map<float, int>::iterator lb = resultMap.lower_bound(prob);
+        if (lb == resultMap.end() ||
+            !resultMap.key_comp()(prob, lb->first)) {
+            resultMap.insert(lb, std::map<float, int>::value_type(prob, classification));
+        }
+    }
+
+    std::cout << "= Prediction values for test ";
+
+    auto it = resultMap.rbegin();
+    for (int i=0; i<5 && it != resultMap.rend(); ++i)
+    {
+        std::cout << "Top(" << (i+1) << ") prediction is " << it->second <<
+            " with confidence: " << 100.0*(it->first) << "%";
+
+        if(mode_type == "onnx")
+            std::cout << "Result is " << label_file_map[it->second+1] << std::endl;
+        else
+            std::cout << "Result is " << label_file_map[it->second] << std::endl;
+
+        ++it;
+    }
+
+    return 0;
+}
+
+void CaculateAvergeDeviation(vector<double>& time_vec)
+{
+    double sum = std::accumulate(time_vec.begin(), time_vec.end(), 0.0);
+    double mean = sum / static_cast<double>(time_vec.size());
+
+    std::vector<double> diff(time_vec.size());
+    std::transform(time_vec.begin(), time_vec.end(), diff.begin(),
+                   std::bind2nd(std::minus<double>(), mean));
+    double sq_sum = std::inner_product(diff.begin(), diff.end(), diff.begin(), 0.0);
+    double stdev = std::sqrt(sq_sum / static_cast<double>(time_vec.size()));
+
+    std::cout << "Total Time Takes " << (sum) << " ms"<< std::endl;
+    std::cout << "Average Time Takes " << (mean) << " ms"<< std::endl;
+    std::cout << "Standard Deviation " << stdev << std::endl;
+}
+
+template<typename TParser, typename TDataType>
+int MainImpl(const char* modelPath,
+             bool isFloatModel,
+             const string mode_type,
+             const char* inputName,
+             const armnn::TensorShape* inputTensorShape,
+             const char* inputTensorDataFilePath,
+             const string inputImageName,
+             const unsigned int inputImageWidth,
+             const unsigned int inputImageHeight,
+             const char* outputName,
+             bool enableProfiling,
+             const size_t subgraphId,
+             const std::shared_ptr<armnn::IRuntime>& runtime = nullptr)
+{
+    // Loads input tensor.
+    std::vector<TDataType> input;
+
+    std::ifstream inputTensorFile(inputTensorDataFilePath);
+    if (!inputTensorFile.good())
+    {
+        std::cout << "Failed to load input tensor data file from " << inputTensorDataFilePath;
+        return EXIT_FAILURE;
+    }
+
+    using TContainer = 
+        mapbox::util::variant<std::vector<float>, std::vector<int>, std::vector<unsigned char>, std::vector<int8_t>>;
+
+    std::vector<TContainer> inputDataContainers;
+    std::vector<TContainer> outputDataContainers;
+
+    std::vector<ImageSet> imageSet =
+    {
+        {inputImageName, 0},
+    };
+
+    try
+    {
+        // Creates an InferenceModel, which will parse the model and load it into an IRuntime.
+        typename InferenceModel<TParser, TDataType>::Params params;
+        params.m_ModelPath = modelPath;
+        params.m_IsModelBinary = true;
+        params.m_InputBindings.push_back(std::string(inputName));
+        params.m_InputShapes.push_back(*inputTensorShape);
+        params.m_OutputBindings.push_back(outputName);
+        params.m_SubgraphId = subgraphId;
+        params.m_ComputeDevices = {armnn::Compute::CpuAcc};
+        InferenceModel<TParser, TDataType> model(params, enableProfiling, "", runtime);
+
+        // Executes the model.
+        std::unique_ptr<ClassifierTestCaseData<TDataType>> TestCaseData;
+
+        if(isFloatModel)
+        {
+            if(mode_type == "onnx")
+            {
+                ImagePreprocessor<TDataType> Image(inputTensorDataFilePath, inputImageWidth, inputImageHeight, imageSet,
+                                                   255.0f, {{0.485f, 0.456f, 0.406f}}, {{0.229f, 0.224f, 0.225f}},
+                                                   ImagePreprocessor<TDataType>::DataFormat::NCHW);
+
+                TestCaseData = Image.GetTestCaseData(0);
+            }
+            else
+            {
+                ImagePreprocessor<TDataType> Image(inputTensorDataFilePath, inputImageWidth, inputImageHeight, imageSet);
+                TestCaseData = Image.GetTestCaseData(0);
+            }
+
+            outputDataContainers.push_back(std::vector<float>(model.GetOutputSize()));
+        }
+        else
+        {
+            std::cout << "Quant Model is loaded" << std::endl;
+            auto inputBinding = model.GetInputBindingInfo();
+            printf("Scale %f\n", inputBinding.second.GetQuantizationScale());
+            printf("Offset %d\n", inputBinding.second.GetQuantizationOffset());
+
+            ImagePreprocessor<TDataType> Image(inputTensorDataFilePath, inputImageWidth, inputImageHeight, imageSet,
+                                               1, {{0, 0, 0}}, {{1, 1, 1}});
+
+            TestCaseData = Image.GetTestCaseData(0);
+
+            outputDataContainers.push_back(std::vector<uint8_t>(model.GetOutputSize()));
+        }
+
+        inputDataContainers.push_back(TestCaseData->m_InputImage);
+
+        //warm up
+	const std::vector<TContainer>& inputRef = inputDataContainers;
+        model.Run(inputRef, outputDataContainers);
+
+        time_point<high_resolution_clock> predictStart;
+        time_point<high_resolution_clock> predictEnd;
+
+        std::vector<double> time_vector;
+
+        for(unsigned int i = 0; i < NUMBER_RUN_TESTS; i++)
+        {
+            predictStart = high_resolution_clock::now();
+
+            model.Run(inputRef, outputDataContainers);
+
+            predictEnd = high_resolution_clock::now();
+
+            double timeTakenS = duration<double>(predictEnd - predictStart).count();
+
+            time_vector.push_back(timeTakenS*1000.0);
+        }
+
+        CaculateAvergeDeviation(time_vector);
+
+        if(isFloatModel)
+        {
+            std::vector<float> output;
+            output = mapbox::util::get<std::vector<float>>(outputDataContainers[0]);
+            ProcessResult<float>(output, model.GetQuantizationParams(), mode_type);
+        }
+        else
+        {
+            std::vector<unsigned char> output;
+            output = mapbox::util::get<std::vector<unsigned char>>(outputDataContainers[0]);
+            ProcessResult<unsigned char>(output, model.GetQuantizationParams(), mode_type);
+        }
+    }
+    catch (armnn::Exception const& e)
+    {
+        std::cout << "Armnn Error: " << e.what();
+        return EXIT_FAILURE;
+    }
+
+    return EXIT_SUCCESS;
+}
+
+void initModelTable()
+{
+   //Basic Model Verification
+    Model_Table["mobilenet_v1_1.0_224_quant.tflite"] = {"tflite-binary", false, base_path + "/tensorflow-lite/models/mobilenet_v1_1.0_224_quant.tflite", armnn::TensorShape({ 1, 224, 224, 3}), "input", "MobilenetV1/Predictions/Reshape_1", 224, 224};
+}
+
+// This will run a test
+template<typename TDataType>
+int RunTest(const std::string& modelFormat,
+            const bool isFloatModel,
+            const armnn::TensorShape& inputTensorShape,
+            const std::string& modelPath,
+            const std::string& inputName,
+            const std::string& inputTensorDataFilePath,
+            const std::string& inputImageName,
+            const unsigned int inputImageWidth,
+            const unsigned int inputImageHeight,
+            const std::string& outputName,
+            bool enableProfiling,
+            const size_t subgraphId,
+            const std::shared_ptr<armnn::IRuntime>& runtime = nullptr)
+{
+    // Parse model binary flag from the model-format string we got from the command-line
+    bool isModelBinary;
+    if (modelFormat.find("bin") != std::string::npos)
+    {
+        isModelBinary = true;
+    }
+    else if (modelFormat.find("txt") != std::string::npos || modelFormat.find("text") != std::string::npos)
+    {
+        isModelBinary = false;
+    }
+    else
+    {
+        std::cout << "Unknown model format: '" << modelFormat << "'. Please include 'binary' or 'text'";
+        return EXIT_FAILURE;
+    }
+
+    // Forward to implementation based on the parser type
+    if (modelFormat.find("onnx") != std::string::npos)
+    {
+#if defined(ARMNN_ONNX_PARSER)
+        return MainImpl<armnnOnnxParser::IOnnxParser, float>(modelPath.c_str(), isFloatModel, "onnx",
+                                                         inputName.c_str(), &inputTensorShape,
+                                                         inputTensorDataFilePath.c_str(), inputImageName,
+                                                         inputImageWidth, inputImageHeight, outputName.c_str(),
+                                                         enableProfiling, subgraphId, runtime);
+#else
+        std::cout << "Not built with Onnx parser support.";
+        return EXIT_FAILURE;
+#endif
+    }
+    else if(modelFormat.find("tflite") != std::string::npos)
+    {
+#if defined(ARMNN_TF_LITE_PARSER)
+        if (! isModelBinary)
+        {
+            std::cout << "Unknown model format: '" << modelFormat << "'. Only 'binary' format supported \
+              for tflite files";
+            return EXIT_FAILURE;
+        }
+        return MainImpl<armnnTfLiteParser::ITfLiteParser, TDataType>(modelPath.c_str(), isFloatModel, "tflite",
+                                                                 inputName.c_str(), &inputTensorShape,
+                                                                 inputTensorDataFilePath.c_str(), inputImageName,
+                                                                 inputImageWidth, inputImageHeight, outputName.c_str(),
+                                                                 enableProfiling, subgraphId, runtime);
+#else
+        std::cout << "Not built with TfLite parser support.";
+        return EXIT_FAILURE;
+
+#endif
+    }
+    else
+    {
+        std::cout << "Unknown model format: '" << modelFormat <<
+                                 "'. Please include 'tflite' or 'onnx'";
+        return EXIT_FAILURE;
+    }
+}
+
+void loadLabelFile(string label_file_name)
+{
+    std::ifstream infile(label_file_name);
+
+    string line;
+    while(std::getline(infile, line))
+    {
+        stringstream line_stream(line);
+        string item;
+        std::vector<string> item_vector;
+        while(std::getline(line_stream, item, ':'))
+        {
+            item_vector.push_back(item);
+        }
+
+        label_file_map[std::stoi(item_vector[0])] = item_vector[1];
+    }
+}
+
+int main()
+{
+    // Configures logging for both the ARMNN library and this test program.
+#ifdef NDEBUG
+    armnn::LogSeverity level = armnn::LogSeverity::Info;
+#else
+    armnn::LogSeverity level = armnn::LogSeverity::Debug;
+#endif
+    armnn::ConfigureLogging(true, true, level);
+
+    initModelTable();
+
+    model_params params;
+
+    bool enableProfiling = false;
+    size_t subgraphId = 0;
+    string inputImageName = "grace_hopper.jpg";
+    string inputImagePath = "/usr/bin/armnn/examples/images/";
+
+    for ( auto it = Model_Table.begin(); it != Model_Table.end(); it++ )
+    {
+        params = Model_Table[it->first];
+
+        //load label file
+        string label_file_name = "/usr/bin/armnn/examples/tensorflow-lite/models/labels.txt";
+
+        loadLabelFile(label_file_name);
+
+        std::cout << "====================" << std::endl;
+        std::cout << "current model is " << it->first << std::endl;
+
+        if(params.isFloatModel)
+        {
+            RunTest<float>(params.modelFormat, params.isFloatModel, params.inputTensorShape, params.modelPath,
+            params.inputName, inputImagePath, inputImageName, params.inputImageWidth,params.inputImageHeight, params.outputName,
+	    enableProfiling, subgraphId);
+        }
+        else
+        {
+            RunTest<uint8_t>(params.modelFormat, params.isFloatModel, params.inputTensorShape, params.modelPath,
+            params.inputName, inputImagePath, inputImageName, params.inputImageWidth,params.inputImageHeight, params.outputName,
+	    enableProfiling, subgraphId);
+        }
+    }
+
+    return 0;
+}
-- 
2.34.1

